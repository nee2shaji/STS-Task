{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "STS Task_Approach2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nlaVJULRFfr"
      },
      "source": [
        "!pip install -q transformers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRegQLFLRLCh"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import transformers\n",
        "import urllib\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xiGDdACIOIj"
      },
      "source": [
        "#  transformers.logging.set_verbosity_error()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ZesbXlqqRLMV",
        "outputId": "3771342d-fd38-4718-f21c-12f37ecbdf1b"
      },
      "source": [
        "# Extract training and test data\n",
        "# For training data, we have about 3000 records from STS 2015\n",
        "# For test data, we have about 9000 records which was given in STS 2016\n",
        "\n",
        "def extract_data(X_train_path, y_train_path):\n",
        "  tmp_df = pd.DataFrame(columns=['sent1', 'sent2', 'y'])\n",
        "  response_X = urllib.request.urlopen(X_train_path)\n",
        "  response_y = urllib.request.urlopen(y_train_path)\n",
        "\n",
        "  for line_X, line_y in zip(response_X, response_y):\n",
        "    if(line_y.decode('utf-8').strip()):\n",
        "      tmp_df = tmp_df.append({'y' : line_y.decode('utf-8').strip(), \n",
        "                              'sent1' : line_X.decode(\"utf-8\").strip().split('\\t')[0],\n",
        "                              'sent2' : line_X.decode(\"utf-8\").strip().split('\\t')[1]}, ignore_index=True)\n",
        "  return tmp_df\n",
        "\n",
        "def extract_test_data(X_train_path, category):\n",
        "  tmp_df = pd.DataFrame(columns=['sent1', 'sent2', 'category'])\n",
        "  response_X = urllib.request.urlopen(X_train_path)\n",
        "\n",
        "  for line_X in response_X:\n",
        "    tmp_df = tmp_df.append({'sent1' : line_X.decode(\"utf-8\").strip().split('\\t')[0],\n",
        "                            'sent2' : line_X.decode(\"utf-8\").strip().split('\\t')[1],\n",
        "                            'category' : category}, ignore_index=True)\n",
        "  return tmp_df\n",
        "  \n",
        "  \n",
        "train_df = pd.DataFrame(columns=['sent1', 'sent2', 'y'])\n",
        "test_df = pd.DataFrame(columns=['sent1', 'sent2', 'category'])\n",
        "\n",
        "train_df = train_df.append(extract_data('https://raw.githubusercontent.com/nee2shaji/STS-Task/main/data/STS.input.answers-forums.txt',\n",
        "                                        'https://raw.githubusercontent.com/nee2shaji/STS-Task/main/data/STS.gs.answers-forums.txt'))\n",
        "train_df = train_df.append(extract_data('https://raw.githubusercontent.com/nee2shaji/STS-Task/main/data/STS.input.answers-students.txt',\n",
        "                                        'https://raw.githubusercontent.com/nee2shaji/STS-Task/main/data/STS.gs.answers-students.txt'))\n",
        "train_df = train_df.append(extract_data('https://raw.githubusercontent.com/nee2shaji/STS-Task/main/data/STS.input.belief.txt',\n",
        "                                        'https://raw.githubusercontent.com/nee2shaji/STS-Task/main/data/STS.gs.belief.txt'))\n",
        "train_df = train_df.append(extract_data('https://raw.githubusercontent.com/nee2shaji/STS-Task/main/data/STS.input.headlines.txt',\n",
        "                                        'https://raw.githubusercontent.com/nee2shaji/STS-Task/main/data/STS.gs.headlines.txt'))\n",
        "train_df = train_df.append(extract_data('https://raw.githubusercontent.com/nee2shaji/STS-Task/main/data/STS.input.images.txt',\n",
        "                                        'https://raw.githubusercontent.com/nee2shaji/STS-Task/main/data/STS.gs.images.txt'))\n",
        "\n",
        "test_df = test_df.append(extract_test_data('https://raw.githubusercontent.com/nee2shaji/STS-Task/main/data/STS2016.input.answer-answer.txt', 0))\n",
        "test_df = test_df.append(extract_test_data('https://raw.githubusercontent.com/nee2shaji/STS-Task/main/data/STS2016.input.headlines.txt', 1))\n",
        "test_df = test_df.append(extract_test_data('https://raw.githubusercontent.com/nee2shaji/STS-Task/main/data/STS2016.input.plagiarism.txt', 2))\n",
        "test_df = test_df.append(extract_test_data('https://raw.githubusercontent.com/nee2shaji/STS-Task/main/data/STS2016.input.postediting.txt', 3))\n",
        "test_df = test_df.append(extract_test_data('https://raw.githubusercontent.com/nee2shaji/STS-Task/main/data/STS2016.input.question-question.txt', 4))\n",
        "\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "train_df['y'] = train_df['y'].astype(float)\n",
        "\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "train_df"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sent1</th>\n",
              "      <th>sent2</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>You'll need to check the particular policies o...</td>\n",
              "      <td>If you need to publish the book and you have f...</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I am not aware of any university run participa...</td>\n",
              "      <td>At the universities I've worked in North Ameri...</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Apart from admission (which normally should no...</td>\n",
              "      <td>Age is not a PhD admission facor in the contin...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This post refers to research in the STEM field...</td>\n",
              "      <td>This is about my experience in computer engine...</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>If you don't want to pay for Adobe Acrobat Pro...</td>\n",
              "      <td>If you don't mind hosting your files online, S...</td>\n",
              "      <td>0.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2995</th>\n",
              "      <td>A baseball player throws the ball.</td>\n",
              "      <td>The basketball player holds the ball.</td>\n",
              "      <td>1.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2996</th>\n",
              "      <td>A man is swinging on a rope over water.</td>\n",
              "      <td>A man in a maroon bathing suit swings on a rop...</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2997</th>\n",
              "      <td>A woman wearing large sunglasses holds newspap...</td>\n",
              "      <td>A fat woman wearing faint blue top is blowing ...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2998</th>\n",
              "      <td>A deer jumps a fence.</td>\n",
              "      <td>A deer is jumping over a fence.</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2999</th>\n",
              "      <td>A young girl dressed in a Minnie mouse outfit ...</td>\n",
              "      <td>a man wearing a white suit holding a newspaper...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3000 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  sent1  ...    y\n",
              "0     You'll need to check the particular policies o...  ...  3.0\n",
              "1     I am not aware of any university run participa...  ...  0.6\n",
              "2     Apart from admission (which normally should no...  ...  1.0\n",
              "3     This post refers to research in the STEM field...  ...  0.6\n",
              "4     If you don't want to pay for Adobe Acrobat Pro...  ...  0.8\n",
              "...                                                 ...  ...  ...\n",
              "2995                 A baseball player throws the ball.  ...  1.6\n",
              "2996            A man is swinging on a rope over water.  ...  3.0\n",
              "2997  A woman wearing large sunglasses holds newspap...  ...  1.0\n",
              "2998                              A deer jumps a fence.  ...  5.0\n",
              "2999  A young girl dressed in a Minnie mouse outfit ...  ...  1.0\n",
              "\n",
              "[3000 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "mMQUU1CjRLPV",
        "outputId": "63aafb7d-8a41-467e-9ba9-c2ad0eb59e40"
      },
      "source": [
        "# Histogram plot of the train labels\n",
        "plt.hist(train_df['y'], bins=6, range=(0, 5.0))\n",
        "plt.show()\n",
        "# type(train_df['y'][0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "If1RWE-vgSoX",
        "outputId": "382880f7-849a-4cc9-da36-14cff9f92e5d"
      },
      "source": [
        "# We round of each train label to nearest int\n",
        "# The distribution of training data in all classes is almost the same\n",
        "train_df['y'] = train_df['y'].apply(lambda x: round(x))\n",
        "train_df['y'].value_counts()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    583\n",
              "1    571\n",
              "3    528\n",
              "4    469\n",
              "0    440\n",
              "5    409\n",
              "Name: y, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSHDh1aWpGPW",
        "outputId": "6bd5caf8-6477-40a9-f0cf-69e9700c2aea"
      },
      "source": [
        "test_df.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9183, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik4EVVu0AQI1",
        "outputId": "c7d356a6-967b-48f9-92f2-940619402558"
      },
      "source": [
        "# The max length of sentence pair is calculated to be used for max_length parameter while tokenising\n",
        "train_sent_len_list = [len(x.split())+len(y.split()) for x,y in zip(train_df['sent1'],train_df['sent2'])]\n",
        "test_sent_len_list = [len(x.split())+len(y.split()) for x,y in zip(test_df['sent1'], test_df['sent2'])]\n",
        "print(max(train_sent_len_list))\n",
        "print(max(test_sent_len_list))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96\n",
            "200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBv55VXgD8HR"
      },
      "source": [
        "# Max length of sentence pair is 200. Hence we set max_length to 256\n",
        "max_length = 256"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzqRRYyQWkNa"
      },
      "source": [
        "# Tokenise the data as required by BERT model returning attention masks and token type ids\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
        "encoded = tokenizer.batch_encode_plus(\n",
        "    train_df[[\"sent1\", \"sent2\"]].values.astype(\"str\").tolist(),\n",
        "    add_special_tokens=True,\n",
        "    max_length=max_length,\n",
        "    return_attention_mask=True,\n",
        "    return_token_type_ids=True,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_tensors=\"tf\")\n",
        "\n",
        "input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
        "attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
        "\n",
        "labels = tf.keras.utils.to_categorical(train_df['y'], num_classes=6)\n",
        "labels = np.array(labels, dtype=\"int32\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aHJrSsXXwaH",
        "outputId": "9c53a98e-dba3-4f23-dedb-428c08d70458"
      },
      "source": [
        "print(input_ids[0])\n",
        "print(attention_masks[0])\n",
        "print(token_type_ids[0])\n",
        "print(input_ids.shape)\n",
        "print(attention_masks.shape)\n",
        "print(token_type_ids.shape)\n",
        "print(labels.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  101  2017  1005  2222  2342  2000  4638  1996  3327  6043  1997  2169\n",
            "  6674  2000  2156  2054  2003  3039  1998  2054  2003  2025  3039  1012\n",
            "   102  2065  2017  2342  2000 10172  1996  2338  1998  2017  2031  2179\n",
            "  2028  6674  2008  4473  2009  1012   102     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0]\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "(3000, 256)\n",
            "(3000, 256)\n",
            "(3000, 256)\n",
            "(3000, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkmckTMsRLSO"
      },
      "source": [
        "# Build the model\n",
        "\n",
        "def build_sts_model():\n",
        "  input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name=\"input_ids\")\n",
        "  attention_masks = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name=\"attention_masks\")\n",
        "  token_type_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\")\n",
        "  \n",
        "  bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "  bert_model.trainable = False\n",
        "\n",
        "  bert_output = bert_model(input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids)\n",
        "  sequence_output = bert_output.last_hidden_state\n",
        "  bi_lstm1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(sequence_output)\n",
        "  bi_lstm2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(bi_lstm1)\n",
        "  bi_lstm3 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(bi_lstm2)\n",
        "  avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm3)\n",
        "  dropout = tf.keras.layers.Dropout(0.3)(avg_pool)\n",
        "  output = tf.keras.layers.Dense(6, activation=\"softmax\")(dropout)\n",
        "  model = tf.keras.models.Model(inputs=[input_ids, attention_masks, token_type_ids], outputs=output)\n",
        "  \n",
        "  model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(),\n",
        "      loss=\"categorical_crossentropy\",\n",
        "      metrics=[\"acc\"])\n",
        "  \n",
        "  return model\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ad67x_NMRLU7",
        "outputId": "3613e5a7-3c86-4317-e7bc-5fe1ac571daf"
      },
      "source": [
        "sts_model = build_sts_model()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNJ9LMQ_a8b9",
        "outputId": "a282ab36-e69e-4255-dd9f-9d25e275d79b"
      },
      "source": [
        "sts_model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_masks (InputLayer)   [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " token_type_ids (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_masks[0][0]',        \n",
            "                                tentions(last_hidde               'token_type_ids[0][0]']         \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 256, 128)     426496      ['tf_bert_model[0][0]']          \n",
            "                                                                                                  \n",
            " bidirectional_1 (Bidirectional  (None, 256, 128)    98816       ['bidirectional[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " bidirectional_2 (Bidirectional  (None, 256, 128)    98816       ['bidirectional_1[0][0]']        \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_average_pooling1d (Glob  (None, 128)         0           ['bidirectional_2[0][0]']        \n",
            " alAveragePooling1D)                                                                              \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 128)          0           ['global_average_pooling1d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 6)            774         ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 110,107,142\n",
            "Trainable params: 624,902\n",
            "Non-trainable params: 109,482,240\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6up7KR-RLXs",
        "outputId": "0cc79321-938a-41bf-ab7e-7ceaa6e67202"
      },
      "source": [
        "# Train the model\n",
        "history = sts_model.fit(\n",
        "    x = [input_ids, attention_masks, token_type_ids], \n",
        "    y = labels,\n",
        "    epochs=5,\n",
        "    use_multiprocessing=True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "94/94 [==============================] - 154s 1s/step - loss: 1.6044 - acc: 0.2970\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 128s 1s/step - loss: 1.3920 - acc: 0.3960\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 128s 1s/step - loss: 1.3197 - acc: 0.4207\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 128s 1s/step - loss: 1.2382 - acc: 0.4653\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 128s 1s/step - loss: 1.1842 - acc: 0.4887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAnVly0SRLum"
      },
      "source": [
        "# Tokenise test data and predict output\n",
        "\n",
        "def get_encoded_test_data(tmp_df):\n",
        "  encoded = tokenizer.batch_encode_plus(\n",
        "      tmp_df[[\"sent1\", \"sent2\"]].values.astype(\"str\").tolist(),\n",
        "      add_special_tokens=True,\n",
        "      max_length=max_length,\n",
        "      return_attention_mask=True,\n",
        "      return_token_type_ids=True,\n",
        "      padding='max_length',\n",
        "      truncation=True,\n",
        "      return_tensors=\"tf\")\n",
        "  \n",
        "  input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
        "  attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "  token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
        "\n",
        "  return [input_ids, attention_masks, token_type_ids]\n",
        "\n",
        "# Evaluation is done separately for each category, hence we output files for each category separately\n",
        "\n",
        "sts_res = sts_model.predict(get_encoded_test_data(test_df[test_df['category']==0]))\n",
        "# print(sts_res.shape)\n",
        "# print(sts_res)\n",
        "pd.Series(np.argmax(sts_res, axis=1)).to_csv('STS2016_output_answer-answer.txt', sep=' ', index=False, header=False)\n",
        "\n",
        "sts_res = sts_model.predict(get_encoded_test_data(test_df[test_df['category']==1]))\n",
        "pd.Series(np.argmax(sts_res, axis=1)).to_csv('STS2016_output_headlines.txt', sep=' ', index=False, header=False)\n",
        "\n",
        "sts_res = sts_model.predict(get_encoded_test_data(test_df[test_df['category']==2]))\n",
        "pd.Series(np.argmax(sts_res, axis=1)).to_csv('STS2016_output_plagiarism.txt', sep=' ', index=False, header=False)\n",
        "\n",
        "sts_res = sts_model.predict(get_encoded_test_data(test_df[test_df['category']==3]))\n",
        "pd.Series(np.argmax(sts_res, axis=1)).to_csv('STS2016_output_postediting.txt', sep=' ', index=False, header=False)\n",
        "\n",
        "sts_res = sts_model.predict(get_encoded_test_data(test_df[test_df['category']==4]))\n",
        "pd.Series(np.argmax(sts_res, axis=1)).to_csv('STS2016_output_question-question.txt', sep=' ', index=False, header=False)"
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}